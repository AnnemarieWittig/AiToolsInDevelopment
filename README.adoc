:warning-caption: :warning:
:toc:
= Extract Repository Data

This repository contains scripts for retrieving data from Git based repositories, with easy adaptability for other Git-based repository managers.

toc::[]

== Analysis

These analysis scripts run without modifications for GitHub, provided it follows the standard setup. If GitLab or Azure is used (in a non-specialized version), they should also work. For other repository managers, response handling and request endpoints may require adjustments due to structural differences. However, the non-API reliant scripts will work with any git repository.

Scripts for analyzing repositories and extracting data:

*  link:/RepositoryCrawlers/generate_commit_data.py[`Commits`]
** Outputs: `{STORAGE_PATH}/commits.csv`
*  link:/RepositoryCrawlers/generate_file_data.py[`Files per commit (modifications)`] _(Must be executed **after** the commit script, as it accesses the commit data)_
** Outputs: `{STORAGE_PATH}/files.json`
*  link:/RepositoryCrawlers/generate_release_data.py[`Releases`]
** Outputs: `{STORAGE_PATH}/releases.csv`
*  link:/RepositoryCrawlers/generate_issue_data.py[`Issues and issue details`]
** Outputs: `{STORAGE_PATH}/issues.csv`
** Bot-generated changes are filtered out. If needed, add bot names to the exclusion list (line 19).
*  link:/RepositoryCrawlers/generate_branch_data.py[`Branches`]
** Outputs: `{STORAGE_PATH}/branches.csv`
*  link:/RepositoryCrawlers/generate_build_data.py[`Builds`]
** Outputs: `{STORAGE_PATH}/workflow_runs.csv`
** Intermediate file _(used as a safeguard in case workflow processing encounters errors, currently deactivated)_: `{STORAGE_PATH}/workflow_runs.json`
*  link:/RepositoryCrawlers/generate_pull_request_data.py[`Pull requests`]
** Outputs: `{STORAGE_PATH}/pull_requests.csv`
** Intermediate file _(used as a safeguard in case workflow processing encounters errors, currently deactivated)_: `{STORAGE_PATH}/pull_requests.json`

Primary scripts are located in `/RepositoryCrawlers`, while console and API interaction functions are in `/RepositoryCrawlers/helpers`. The generated files serve as the dataset for analysis.

It is recommended to exectue scripts in the listed order for dependencies. However, apart from interdependencies, execution order is flexible. Execution time varies depending on repository size. A shell script automates execution and is detailed in the link:#_setup_for_automatic_activation_of_scripts[Setup for Automatic Activation of Scripts].

Scripts use the stored users and emails, retrieved via the git command line, to pseudomizes any personal mentions of developers, creators, etc. using the hashed email-address. In some cases, git does not know the usernames used and thus cannot pseudomize them.

== Setup 
=== For Automatic Activation of Scripts

To execute scripts the scripts automatically, ensure the following prerequisites:

* *List with the repositories* (as detailed below)
* **Internet connection** (for API access)
* **Locally cloned repositories**
* **The python package** `virtualenv` as it is used to automatically create the environment and install the dependencies (check via `python -m venv --help`, install with `pip install virtualenv`). _This is usually installed by default._
** If you do not want to use python virtualenv, the automatic scripts need to be adjusted accordingly.

Then, you follow these steps:

. To process multiple repositories automatically, add their details to `file_list.csv`.
   The file structure:
+
[source,bash]
----
ACCESS_TOKEN, REPO_PATH, STORAGE_PATH, OWNER, REPO, MAIN_BRANCH, ENDPOINT, MODE, PROJECT
----
+
There is no header expected, so the script will read the first line as a repository to extract data from. Example entries can be found in link:./file_list.csv[file_list.csv]. Each field is explained in link:#environment-values[the section on environment variables]. *Ensure the file ends with an empty line* to allow complete reading by the script.

**Windows: Powershell**

. Start the script from the repository root:
+
[source,bash]
----
.\run_multiple_windows.ps1
----

**Mac / Linux: Bash **

. Start the script from the repository root:
+
[source,bash]
----
bash ./run_multiple.sh
----

TIP: Here is what happens:   
1. A Python virtual environment is created at `current-directory/venv`, and required dependencies are installed.  
2. A folder named after each repository is added to the designated storage directory. *Repositories cannot have the same name within the same storage folder.*  
3. Extracted files and a log file (only with the bash script) are stored in their respective directories.  

== Setup for Manual Activation of Scripts

To execute scripts manually, ensure the following prerequisites:

*  **Environment variables** (as detailed above)
*  **Internet connection** (for API access)
*  **Locally cloned repository**

Create a `.env` file in the root directory containing the required values. Additionally, include `VIRTUAL_ENVIRONMENT_PATH` specifying the full path to the Python environment.

For private git instances, API URL definitions may need adjustments in link:/RepositoryCrawlers/helper/api_access.py[`api_access.py`].

== Environment Values

The scripts rely on the following environment variables:

*  `ACCESS_TOKEN`: The access token to your repository manager; They are required to have at minimum read access.
*  `REPO_PATH`: Local path to the cloned repository for analysis
*  `STORAGE_PATH`: Directory for storing results
*  `OWNER`: Value differs depending on the repository manager:
** _Github_: Repository owner 
** _Gitlab_: Project ID, found in your repository settings, under _General_
** _Azure Repos_: Azure DevOps organization your project is located at (not the project name, in my case it would be AnnemarieWittig)
** _Bitbucket_: Not relevant, can be left empty
*  `REPO`: Repository name
*  `MAIN_BRANCH`: Main branch (typically `main`, but varies)
*  `ENDPOINT`: API endpoint of the repository manager (e.g., `https://api.github.com` for GitHub)
*  `MODE`: Repository manager mode (`github`, `gitlab` or `azure` only)
*  `PROJECT`: Value differs depending on the repository manager:
** _Github_ and _Gitlab_: Not relevant, can be left empty
** _Azure_: the project name (not the repository!)
** _Bitbucket_: the project key, can be found in the URL of your project: `https://my-bitbucket.com/projects/{PROJECT KEY}/repos/{REPOSITORY NAME}/browse`


WARNING: Without the variables, the data retrieval will not work.

== Data Considerations

=== Mode Restrictions

Some of the data we extract might look different or be missing depending on the mode. Those are usually marked as `Not/{MODE}`.

=== Azure Restrictions

Some of the data we retrieve via API (issues / work items, workflows) are set up as part of an azure project, not repository. Thus, we retrieve all issues in the connected project, and not just for the repository.

=== Workflows

Certain workflows may lack a triggering actor due to various reasons. The triggering event usually determines the actor presence. Below is an overview:

[options="header",cols="2,1,1"]
|===
| Event (`run["event"]`) | Expected `triggering_actor`? | Possible Missing Actor?
| `push` | pass:[&#10004;] User who pushed | pass:[&#10008;] If a bot pushed (e.g., `github-actions[bot]`)
| `pull_request` | pass:[&#10004;] User who opened PR | pass:[&#10008;] If PR originates from a **fork** with restricted permissions
| `workflow_dispatch` | pass:[&#10004;] User who triggered manually | pass:[&#10008;] If triggered via API without a user
| `repository_dispatch` | pass:[&#10008;] External system trigger | pass:[&#10004;] No actor (unless explicitly set in API request)
| `schedule` | pass:[&#10008;] Cron job trigger | pass:[&#10004;] No actor (GitHub Actions runs it)
| `workflow_run` | pass:[&#10008;] Triggered by another workflow | pass:[&#10004;] No actor (automated process)
| `deployment` | pass:[&#10004;] User or bot initiating a deployment | pass:[&#10008;] If triggered by a bot
| `release` | pass:[&#10004;] User who created release | pass:[&#10008;] If done by a bot
| `issue_comment` | pass:[&#10004;] User who commented | pass:[&#10008;] If triggered via API without a user
| `pull_request_review` | pass:[&#10004;] Reviewer | pass:[&#10008;] If triggered by automation
| `merge_group` | pass:[&#10004;] User merging multiple PRs | pass:[&#10008;] If GitHub initiates merge
|===

This table highlights when actors are expected and when they may be missing due to automation or API restrictions.