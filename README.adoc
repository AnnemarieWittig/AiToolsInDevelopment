:warning-caption: :warning:
:toc:
= Extract Repository Data

This repository contains scripts for retrieving data from GitHub repositories, with easy adaptability for other Git-based repository managers.

toc::[]

== Analysis

These analysis scripts run without modifications for GitHub, provided it follows the standard setup. If GitLab is used (in a non-specialized version), they should also work. For other repository managers, response handling may require adjustments due to structural differences.

Scripts for analyzing repositories and extracting data:

*  link:/RepositoryCrawlers/generate_commit_data.py[`Commits`]
** Outputs: `{STORAGE_PATH}/commits.csv`
*  link:/RepositoryCrawlers/generate_file_data.py[`Files per commit (modifications)`] _(Must be executed **after** the commit script, as it accesses the commit data)_
** Outputs: `{STORAGE_PATH}/files.json`
*  link:/RepositoryCrawlers/generate_release_data.py[`Releases`]
** Outputs: `{STORAGE_PATH}/releases.csv`
*  link:/RepositoryCrawlers/generate_issue_data.py[`Issues and issue details`]
** Outputs: `{STORAGE_PATH}/issues.csv`
** Bot-generated changes are filtered out. If needed, add bot names to the exclusion list (line 19).
*  link:/RepositoryCrawlers/generate_branch_data.py[`Branches`]
** Outputs: `{STORAGE_PATH}/branches.csv`
*  link:/RepositoryCrawlers/generate_build_data.py[`Builds`]
** Outputs: `{STORAGE_PATH}/workflow_runs.csv`
** Intermediate file _(used as a safeguard in case workflow processing encounters errors, currently deactivated)_: `{STORAGE_PATH}/workflow_runs.json`
*  link:/RepositoryCrawlers/generate_pull_request_data.py[`Pull requests`]
** Outputs: `{STORAGE_PATH}/pull_requests.csv`
** Intermediate file _(used as a safeguard in case workflow processing encounters errors, currently deactivated)_: `{STORAGE_PATH}/pull_requests.json`

Primary scripts are located in `/RepositoryCrawlers`, while console and API interaction functions are in `/RepositoryCrawlers/helpers`. The generated files serve as the dataset for analysis.

It is recommended to exectue scripts in the listed order for dependencies. However, apart from interdependencies, execution order is flexible. Execution time varies depending on repository size. A shell script automates execution and is detailed in the link:#_setup_for_automatic_activation_of_scripts[Setup for Automatic Activation of Scripts].

Scripts use the stored users and emails, retrieved via the git command line, to pseudomizes any personal mentions of developers, creators, etc. using the hashed email-address. In some cases, git does not know the usernames used and thus cannot pseudomize them.

== Setup 
=== For Automatic Activation of Scripts

To execute scripts the scripts automatically, ensure the following prerequisites:

* *List with the repositories* (as detailed below)
* **Internet connection** (for GitHub API access)
* **Locally cloned repositories**

Then, you follow these steps:

. To process multiple repositories automatically, add their details to `file_list.csv`.
   The file structure:
+
[source,bash]
----
ACCESS_TOKEN, REPO_PATH, STORAGE_PATH, OWNER, REPO, MAIN_BRANCH, ENDPOINT, MODE, PROJECT
----
+
Example entries can be found in link:./file_list.csv[file_list.csv]. Each field is explained in link:#environment-values[the section on environment variables].. *Ensure the file ends with an empty line* to allow complete reading by the script.

. Start the script from the repository root:
+
[source,bash]
----
bash ./run_multiple.sh
----

TIP: Here is what happens:  
1. A Python virtual environment is created at `current-directory/venv`, and required dependencies are installed.
2. A folder named after each repository is added to the designated storage directory. *Repositories cannot have the same name within the same storage folder.*
3. Generated files and a log file that stores the `run_multiple.sh` output are stored in their respective directories.

== Setup for Manual Activation of Scripts

To execute scripts manually, ensure the following prerequisites:

*  **Environment variables** (as detailed above)
*  **Internet connection** (for GitHub API access)
*  **Locally cloned repository**

Create a `.env` file in the root directory containing the required values. Additionally, include `VIRTUAL_ENVIRONMENT_PATH` specifying the full path to the Python environment.

For private GitHub instances, API URL definitions may need adjustments in link:/RepositoryCrawlers/helper/api_access.py[`api_access.py`].

== Environment Values

The scripts rely on the following environment variables:

*  `ACCESS_TOKEN`: GitHub access token
*  `REPO_PATH`: Local path to the cloned repository for analysis
*  `STORAGE_PATH`: Directory for storing results
*  `OWNER`: Value differs depending on the repository manager:
** _Github_: Repository owner 
** _Gitlab_: Project ID, found in your repository settings, under _General_
** _Azure Repos_: Azure DevOps organization your project is located at (not the project name, in my case it would be AnnemarieWittig)
*  `REPO`: Repository name
*  `MAIN_BRANCH`: Main branch (typically `main`, but varies)
*  `ENDPOINT`: API endpoint of the repository manager (e.g., `https://api.github.com` for GitHub)
*  `MODE`: Repository manager mode (`github`, `gitlab` or `azure` only)
*  `PROJECT`: Only relevant for Azure, represents the project name (not the repository!); can be left empty for other MODEs

WARNING: Without the variables, the data retrieval will not work.

== Data Considerations

=== Mode Restrictions

Some of the data we extract might look different or be missing depending on the mode. Those are usually marked as `Not/{MODE}`.

=== Azure Restrictions

Some of the data we retrieve via API (issues / work items, workflows) are set up as part of an azure project, not repository. Thus, we retrieve all issues in the connected project, and not just for the repository.

=== Workflows

Certain workflows may lack a triggering actor due to various reasons. The triggering event usually determines the actor presence. Below is an overview:

[options="header",cols="2,1,1"]
|===
| Event (`run["event"]`) | Expected `triggering_actor`? | Possible Missing Actor?
| `push` | pass:[&#10004;] User who pushed | pass:[&#10008;] If a bot pushed (e.g., `github-actions[bot]`)
| `pull_request` | pass:[&#10004;] User who opened PR | pass:[&#10008;] If PR originates from a **fork** with restricted permissions
| `workflow_dispatch` | pass:[&#10004;] User who triggered manually | pass:[&#10008;] If triggered via API without a user
| `repository_dispatch` | pass:[&#10008;] External system trigger | pass:[&#10004;] No actor (unless explicitly set in API request)
| `schedule` | pass:[&#10008;] Cron job trigger | pass:[&#10004;] No actor (GitHub Actions runs it)
| `workflow_run` | pass:[&#10008;] Triggered by another workflow | pass:[&#10004;] No actor (automated process)
| `deployment` | pass:[&#10004;] User or bot initiating a deployment | pass:[&#10008;] If triggered by a bot
| `release` | pass:[&#10004;] User who created release | pass:[&#10008;] If done by a bot
| `issue_comment` | pass:[&#10004;] User who commented | pass:[&#10008;] If triggered via API without a user
| `pull_request_review` | pass:[&#10004;] Reviewer | pass:[&#10008;] If triggered by automation
| `merge_group` | pass:[&#10004;] User merging multiple PRs | pass:[&#10008;] If GitHub initiates merge
|===

This table highlights when actors are expected and when they may be missing due to automation or API restrictions.